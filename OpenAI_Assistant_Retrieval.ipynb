{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOblK-sMY1-k",
        "outputId": "8938d283-b116-4961-e71f-32f86b824e64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.3.5)\n",
            "Collecting cohere\n",
            "  Downloading cohere-4.36-py3-none-any.whl (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken\n",
            "  Downloading tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai) (4.5.0)\n",
            "Requirement already satisfied: aiohttp<4.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (3.8.6)\n",
            "Collecting backoff<3.0,>=2.0 (from cohere)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Collecting fastavro==1.8.2 (from cohere)\n",
            "  Downloading fastavro-1.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib_metadata<7.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (6.8.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.25.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.31.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.0.7)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.1.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib_metadata<7.0,>=6.0->cohere) (3.17.0)\n",
            "Installing collected packages: fastavro, backoff, tiktoken, cohere\n",
            "Successfully installed backoff-2.2.1 cohere-4.36 fastavro-1.8.2 tiktoken-0.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip install openai cohere tiktoken --upgrade"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. CREATE CLIENT AND LOAD THE FILE"
      ],
      "metadata": {
        "id": "9hdbgmmLZDX1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(api_key = \"api-key\")"
      ],
      "metadata": {
        "id": "aWiAsM9KZBfA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file = client.files.create(\n",
        "    file = open(\"/content/cs224u-contextualreps-2023-handout.pdf\", \"rb\"),\n",
        "    purpose = \"assistants\"\n",
        ")"
      ],
      "metadata": {
        "id": "Z4fICXEsZoLM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(file.id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JcGRoNNaB_t",
        "outputId": "9fdc40e9-e9e8-40d3-f239-bbe06ee04c75"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "file-WtbKRqh3g1jZ7vDGcZPM5eJf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. CREATE ASSISTANT"
      ],
      "metadata": {
        "id": "RC8k1L_aaGXm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "assistant = client.beta.assistants.create(\n",
        "    name = \"Machine learning researcher\",\n",
        "    instructions = \"You are a Machine learning researcher, answer questions on the given lecture.\",\n",
        "    tools = [{\"type\": \"retrieval\"}],\n",
        "    model = \"gpt-4-1106-preview\",\n",
        "    file_ids = [\"file-WtbKRqh3g1jZ7vDGcZPM5eJf\"]\n",
        ")"
      ],
      "metadata": {
        "id": "WzL8teVDaEIC"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. CREATE THREADS"
      ],
      "metadata": {
        "id": "3XoWzC6hcLeH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "thread = client.beta.threads.create()\n",
        "print(thread)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKQobnUWbFHB",
        "outputId": "22e2812e-b8fe-4037-9364-8fd14fc2d5d6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thread(id='thread_8MW1yQfi4I5chyWvCaBVjTiz', created_at=1701066146, metadata={}, object='thread')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.CREATE MESSAGE"
      ],
      "metadata": {
        "id": "GNyfKS-OcZgG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "message = client.beta.threads.messages.create(\n",
        "    thread_id = thread.id,\n",
        "    role = \"user\",\n",
        "    content = \"What is postional encoding?\"\n",
        ")\n",
        "print(message)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5mXZOKrcZB2",
        "outputId": "82a315a5-42d8-4e27-bb81-d83cb9b20723"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ThreadMessage(id='msg_gAJloxjkNNJMBpi4yb13TQQb', assistant_id=None, content=[MessageContentText(text=Text(annotations=[], value='What is postional encoding?'), type='text')], created_at=1701066257, file_ids=[], metadata={}, object='thread.message', role='user', run_id=None, thread_id='thread_8MW1yQfi4I5chyWvCaBVjTiz')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.RUN THE ASSISTANT"
      ],
      "metadata": {
        "id": "UFLLJkBSc3OY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run = client.beta.threads.runs.create(\n",
        "    thread_id = thread.id,\n",
        "    assistant_id = assistant.id\n",
        ")\n",
        "print(run)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pMMh40EcYCh",
        "outputId": "ab3518da-17d2-4d82-c8a5-b2539c134645"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run(id='run_8J9z8AnQvwRLrwJU7oRPoman', assistant_id='asst_ZT4Rw9iXsALUfu4KdMWk8bQd', cancelled_at=None, completed_at=None, created_at=1701066471, expires_at=1701067071, failed_at=None, file_ids=['file-WtbKRqh3g1jZ7vDGcZPM5eJf'], instructions='You are a Machine learning researcher, answer questions on the given lecture.', last_error=None, metadata={}, model='gpt-4-1106-preview', object='thread.run', required_action=None, started_at=None, status='queued', thread_id='thread_8MW1yQfi4I5chyWvCaBVjTiz', tools=[ToolAssistantToolsRetrieval(type='retrieval')])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.DISPLAY THE ASSISTANT RESPONSE"
      ],
      "metadata": {
        "id": "swJlFtRidUsJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run = client.beta.threads.runs.retrieve(\n",
        "    thread_id = thread.id,\n",
        "    run_id = run.id\n",
        ")\n",
        "\n",
        "messages = client.beta.threads.messages.list(\n",
        "    thread_id = thread.id\n",
        ")"
      ],
      "metadata": {
        "id": "Tz_0y6QsdSG7"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for message in reversed(messages.data):\n",
        "  print(message.role + \": \" + message.content[0].text.value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibkt_8AEd2YM",
        "outputId": "78fbc9b3-548c-4fdc-c303-1be10206901d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user: What is postional encoding?\n",
            "assistant: Positional encoding is a mechanism used in transformer models to account for the order of words, or more generally, the positions of elements within a sequence. Since the self-attention mechanism in transformers does not inherently encode the sequential nature of the input data, positional encodings are added to ensure that the model can delineate the order of the input tokens.\n",
            "\n",
            "Here are the key points related to positional encoding based on the handout provided:\n",
            "\n",
            "- The Transformer's capacity to keep track of word order is limited since the attention connections are not directional, and there are no other interactions between the columns of the model. Positional encoding is used to overcome this limitation, ensuring that differing input sequences like \"A B C\" versus \"C B A\" are distinguishable by the model【20†source】.\n",
            "\n",
            "- Positional encodings are also useful in representing hierarchical notions of position, such as in tasks like Natural Language Inference where the understanding of premise and hypothesis relationships might be critical【20†source】.\n",
            "\n",
            "- When evaluating positional encoding schemes, one should consider whether the set of positions needs to be decided ahead of time and whether the scheme impedes generalization to new positions【21†source】.\n",
            "\n",
            "- Absolute positional encoding schemes have limitations, such as requiring the set of positions to be predetermined and potentially hindering generalization to new positions【22†source】.\n",
            "\n",
            "- Frequency-based positional encoding also shares the same limitations of needing predefined positions and hindering generalization【23†source】.\n",
            "\n",
            "- Relative positional encoding, as proposed by Shaw et al. in 2018, attempts to address some of these limitations by incorporating proximity information in the encoding process, allowing the model to account for the relative positions of tokens within certain windows or contexts【24†source】.\n",
            "\n",
            "- The full definition of relative positional encoding involves learned attention parameters and adds positional information directly to the attention mechanism, although it still requires predefining the positions and may face generalization issues【26†source】.\n",
            "\n",
            "In contexts where the full details of the mechanisms are discussed, absolute positional encoding directly attaches a positional marker to each input token, while relative positional encoding considers the relative distances between tokens. The specific methodology can vary between different transformer models and applications.\n",
            "assistant: Positional encoding is a mechanism used in transformer models to account for the order of words, or more generally, the positions of elements within a sequence. Since the self-attention mechanism in transformers does not inherently encode the sequential nature of the input data, positional encodings are added to ensure that the model can delineate the order of the input tokens.\n",
            "\n",
            "Here are the key points related to positional encoding based on the handout provided:\n",
            "\n",
            "- The Transformer's capacity to keep track of word order is limited since the attention connections are not directional, and there are no other interactions between the columns of the model. Positional encoding is used to overcome this limitation, ensuring that differing input sequences like \"A B C\" versus \"C B A\" are distinguishable by the model【20†source】.\n",
            "\n",
            "- Positional encodings are also useful in representing hierarchical notions of position, such as in tasks like Natural Language Inference where the understanding of premise and hypothesis relationships might be critical【20†source】.\n",
            "\n",
            "- When evaluating positional encoding schemes, one should consider whether the set of positions needs to be decided ahead of time and whether the scheme impedes generalization to new positions【21†source】.\n",
            "\n",
            "- Absolute positional encoding schemes have limitations, such as requiring the set of positions to be predetermined and potentially hindering generalization to new positions【22†source】.\n",
            "\n",
            "- Frequency-based positional encoding also shares the same limitations of needing predefined positions and hindering generalization【23†source】.\n",
            "\n",
            "- Relative positional encoding, as proposed by Shaw et al. in 2018, attempts to address some of these limitations by incorporating proximity information in the encoding process, allowing the model to account for the relative positions of tokens within certain windows or contexts【24†source】.\n",
            "\n",
            "- The full definition of relative positional encoding involves learned attention parameters and adds positional information directly to the attention mechanism, although it still requires predefining the positions and may face generalization issues【26†source】.\n",
            "\n",
            "In contexts where the full details of the mechanisms are discussed, absolute positional encoding directly attaches a positional marker to each input token, while relative positional encoding considers the relative distances between tokens. The specific methodology can vary between different transformer models and applications.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x7S5BVO4eIdP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}